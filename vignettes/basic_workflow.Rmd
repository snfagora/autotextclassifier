---
title: "Basic workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{basic_workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  rmarkdown.html_vignette.check_title = FALSE
)
```

## Load libraries 

```{r eval = FALSE}
library(autotextclassifier) # Auto text classifier 
library(parallel) # Parallel processing 
library(doParallel) # Parallel processing 
library(here) # Creating reproducible file paths 
library(patchwork) # Putting ggplots together
library(recipes) # Preprocessing 
library(zeallot) # Multiple assignments 
library(yardstick) # Metrics  
```

## Import data 

```{r eval = FALSE}
load(file = here("inst/extdata/sample_data.rda"))
```

## Data munging 

Don't forget to make sure that the type of the outcome variable should be factor.

```{r eval = FALSE}
names(sample_data) <- c("category", "org_name", "ein", "text")
sample_data$category <- as.factor(sample_data$category)
```

## Apply basic recipe 

The `rec` object provides the following information. The function also checks whether the `text` column has missing values or includes extremely short documents (less than five words).

There are two broad basic options for text preprocessing. 

1. Without word embedding 

* Tokenization for text [trained]
* Stop word removal for text [trained]
* Text filtering for text [trained]
* Term frequency-inverse document frequency with text [trained]

2. With word embedding 

* Tokenization for text [trained]
* Stop word removal for text [trained]
* Text filtering for text [trained]
* Word embeddings aggregated from text [trained]

```{r eval = FALSE}
# Without word embedding 
rec <- apply_basic_recipe(sample_data, category ~ text, text)

# With word embedding 
rec_alt <- apply_basic_recipe(sample_data, category ~ text, text, add_embedding = TRUE)
```

## Split data 

```{r eval = FALSE}
set.seed(1234)

c(train_x_class, test_x_class, train_y_class, test_y_class) %<-% split_using_srs(data = sample_data, category = category, rec = rec)
```

## Create tuning parameters 

```{r eval = FALSE}
c(lasso_spec, rand_spec, xg_spec) %<-% create_tunes()
```

## Create search spaces 

```{r eval = FALSE}
c(lasso_grid, rand_grid, xg_grid) %<-% create_search_spaces(train_x_class, category, lasso_spec, rand_spec, xg_spec)
```

## Create workflows 

```{r eval = FALSE}
c(lasso_wf, rand_wf, xg_wf) %<-% create_workflows(lasso_spec, rand_spec, xg_spec, category)
```

## Create 10-fold cross-validation samples 

```{r eval = FALSE}
set.seed(1234)

class_folds <- create_cv_folds(train_x_class, train_y_class, category)
```

## Find the best model from each algorithm

Consider using parallel processing to speed up.

```{r eval = FALSE}
all_cores <- parallel::detectCores(logical = FALSE)

cl <- makeCluster(all_cores[1] - 1)

registerDoParallel(cl)
```

The default metric for model evaluation is accuracy. The other options are balanced accuracy (bal_accuracy), F-score (f_means), and Area under the ROC curve (roc_auc).

```{r eval = FALSE}
c(best_lasso, best_rand, best_xg) %<-% find_best_model(lasso_wf, rand_wf, xg_wf, class_folds, lasso_grid, rand_grid, xg_grid, metric_choice = "accuracy")
```

## Fit the best model from each algorithm to the data 

This step would take the longest running time. We recommend saving this output as an RData object somewhere.

```{r eval = FALSE}
c(lasso_fit, rand_fit, xg_fit) %<-% fit_best_model(lasso_wf, best_lasso, rand_wf, best_rand, xg_wf, best_xg, train_x_class, train_y_class, category)
```

## Evaluate the model using visualization

```{r eval = FALSE}
# Show the results 
viz_class_fit(lasso_fit, "Lasso", test_x_class, test_y_class) + 
viz_class_fit(rand_fit, "Random forest", test_x_class, test_y_class) +
viz_class_fit(xg_fit, "XGBoost", test_x_class, test_y_class)
```
