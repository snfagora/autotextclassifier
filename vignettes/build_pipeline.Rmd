---
title: "Build a pipeline"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{build_pipeline}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  rmarkdown.html_vignette.check_title = FALSE
)
```

## Load libraries 

```{r eval = FALSE}
library(autotextclassifier) # Auto text classifier 
library(parallel) # Parallel processing 
library(doParallel) # Parallel processing 
library(here) # Creating reproducible file paths 
library(patchwork) # Putting ggplots together
library(recipes) # Preprocessing 
library(zeallot) # Multiple assignments 
library(yardstick) # Metrics  
```

## Import data 

```{r eval = FALSE}
load(file = here("inst/extdata/sample_data.rda"))
```

## Data munging 

Don't forget to make sure that the type of the outcome variable should be factor.

```{r eval = FALSE}
names(sample_data) <- c("category", "org_name", "ein", "text")
sample_data$category <- as.factor(sample_data$category)
```

## Apply basic recipe 

The `rec` object provides the following information. The function also checks whether the `text` column has missing values or includes extremely short documents (less than five words).

> Training data contained 232 data points and no missing data.

> Operations:

> Tokenization for text [trained]
Stop word removal for text [trained]
Text filtering for text [trained]
Term frequency-inverse document frequency with text [trained]

```{r eval = FALSE}
rec <- apply_basic_recipe(sample_data, category ~ text, text)
```

## Build a pipeline 

The `build_pipeline` function reduces the steps one needs to take a classifier pipeline. The pipeline involves data splitting, creating tuning parameters, search spaces, workflows, 10-fold cross-validation samples, finding the best model from each algorithm and fitting the best model from each algorithm to the data. 

```{r eval = FALSE}
c(lasso_fit, rand_fit, xg_fit) %<-% build_pipeline(rec, category, rec, prop_ratio = 0.8, metric_choice = "accuracy")
```

## Evaluate the model using visualization

```{r eval = FALSE}
# Decide the metrics to display 
metrics <- yardstick::metric_set(accuracy, bal_accuracy, f_meas, roc_auc)

# Show the results 
viz_class_fit(lasso_fit, "Lasso", test_x_class, test_y_class) + 
viz_class_fit(rand_fit, "Random forest", test_x_class, test_y_class) +
viz_class_fit(xg_fit, "XGBoost", test_x_class, test_y_class)
```
